{{ if .Values.spec.kubedownscaler.enabled -}}
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: kube-downscaler{{ .Values.spec.destination.clustername }}
  namespace: argocd
spec:
  syncPolicy:
    automated: # automated sync by default retries failed attempts 5 times with following delays between attempts ( 5s, 10s, 20s, 40s, 80s ); retry controlled using `retry` field.
      prune: true # Specifies if resources should be pruned during auto-syncing ( false by default ).
      selfHeal: true # Specifies if partial app sync should be executed when resources are changed only in target Kubernetes cluster and no git change detected ( false by default ).
      allowEmpty: false # Allows deleting all application resources during automatic syncing ( false by default ).
    syncOptions:     # Sync options which modifies sync behavior
      - Validate=false # disables resource validation (equivalent to 'kubectl apply --validate=false') ( true by default ).
      - CreateNamespace=true # Namespace Auto-Creation ensures that namespace specified as the application destination exists in the destination cluster.
      - PrunePropagationPolicy=foreground # Supported policies are background, foreground and orphan.
      - PruneLast=true # Allow the ability for resource pruning to happen as a final, implicit wave of a sync operation
    retry:
      limit: 5 # number of failed sync attempt retries; unlimited number of attempts if less than 0
      backoff:
        duration: 5s # the amount to back off. Default unit is seconds, but could also be a duration (e.g. "2m", "1h")
        factor: 2 # a factor to multiply the base duration after each failed retry
        maxDuration: 3m # the maximum amount of time allowed for the backoff strategy
  destination:
    namespace: {{ .Values.spec.destination.project }}
    server: {{ .Values.spec.destination.server }}
  project: {{ .Values.spec.destination.project }}
  source:
    chart: kube-downscaler
    repoURL: https://hahow-helm-charts.storage.googleapis.com/
    targetRevision: 0.1.2
    helm:
      values: |
        replicaCount: 1
        image:
          repository: tools.standardbank.co.za:8092/hjacobs/kube-downscaler
          pullPolicy: IfNotPresent

        imagePullSecrets: []
        nameOverride: ""
        fullnameOverride: ""

        serviceAccount:
          create: true
          annotations: {}
          name:

        podSecurityContext: {}

        securityContext: {}

        resources:
          requests:
            cpu: 10m
            memory: 100Mi
          limits:
            cpu: 10m
            memory: 100Mi

        nodeSelector: {}

        tolerations: []

        affinity: {}

        rbac:
          create: true

        ## Dry run mode: do not change anything, just print what would be done
        dryRun: false

        ## Debug mode: print more information
        debug: false

        ## Run loop only once and exit
        once: false

        ## Loop interval (default: 30s)
        interval: 30

        namespace:
          ## Restrict the downscaler to work only in a single namespace
          ## (default: all namespaces). This is mainly useful for deployment scenarios
          ## where the deployer of kube-downscaler only has access to a given namespace
          ## (instead of cluster access).
          activeIn:
          ## Exclude namespaces from downscaling (default: kube-system)
          inactiveIn:
          # - kube-system

        ## Downscale resources of this kind. (default: deployments)
        includeResources:
        # - deployments
        # - statefulsets
        # - cronjobs
        # - horizontalpodautoscalers

        ## Grace period in seconds for new deployments before scaling them down
        ## (default: 15min). The grace period counts from time of creation of the
        ## deployment, i.e. updated deployments will immediately be scaled down
        ## regardless of the grace period.
        gracePeriod: # 900

        ## Alternative logic to scale up only in given period of time (default: never),
        ## can also be configured via the annotation downscaler/upscale-period on each
        ## deployment.
        upscalePeriod: # never

        ## Alternative logic to scale down only in given period of time
        ## (default: never), can also be configured via the annotation
        ## downscaler/downscale-period on each deployment
        downscalePeriod: # never

        ## Default time range to scale up for (default: always), can also be
        ## configured via the annotation downscaler/uptime on each deployment.
        defaultUptime: # always

        ## Default time range to scale down for (default: never), can also be
        ## configured via the annotation downscaler/downtime on each deployment.
        defaultDowntime: # never

        ## Exclude specific deployments/statefulsets/cronjobs from downscaling
        ## (default:kube-downscaler, downscaler). Despite its name, this option will
        ## match the name of any included resource type (Deployment, StatefulSet,
        ## CronJob, ..).
        excludeDeployments:
        # - kube-downscaler
        # - downscaler

        ## Default value of replicas to downscale to, the annotation
        ## downscaler/downtime-replicas takes precedence over this value.
        downtimeReplicas: # 0

        ## Optional: name of the annotation that would be used instead of the creation
        ## timestamp of the resource. This option should be used if you want the
        ## resources to be kept scaled up during a grace period (--grace-period) after
        ## a deployment. The format of the annotation's timestamp value must be exactly
        ## the same as for Kubernetes' creationTimestamp: %Y-%m-%dT%H:%M:%SZ.
        ## Recommended: set this annotation by your deployment tooling automatically.
        deploymentTimeAnnotation:

{{- end }}